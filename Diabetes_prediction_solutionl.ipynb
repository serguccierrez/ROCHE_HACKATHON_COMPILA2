{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time \n",
        "\n",
        "#------------------------------------------[DATA EXTRACTION]------------------------------------------------\n",
        "\n",
        "# Definitions and initializations\n",
        "BASE_URL = \"https://api.hackupm2025.workers.dev\"\n",
        "train_list_endpoint = \"/api/v1/patients/train\"\n",
        "i = 1\n",
        "lista=[]\n",
        "while True:\n",
        "    try:\n",
        "        params_consulta = {\n",
        "            'page': i,\n",
        "            'limit': 20,\n",
        "        }\n",
        "\n",
        "        url_completa = BASE_URL + train_list_endpoint\n",
        "\n",
        "        # Launch the GET request to the API\n",
        "        response = requests.get(url_completa, params=params_consulta, timeout=10)\n",
        "\n",
        "        # If the response is successful, process the data\n",
        "        if response.status_code == 200:\n",
        "            datos = response.json()\n",
        "\n",
        "            # Process and store the data in 'lista'\n",
        "            for fila in datos['data']:\n",
        "                lista.append((fila['patient_id'], fila['has_diabetes'], fila['medical_note']))\n",
        "           \n",
        "            # We check if there are more pages to fetch or not to make sure we've extracted all data\n",
        "            if not datos[\"pagination\"][\"hasNextPage\"]:\n",
        "                print(\"No hay más páginas. Saliendo del bucle.\")\n",
        "                break \n",
        "            \n",
        "            # Increment the page number for the next iteration\n",
        "            i += 1\n",
        "\n",
        "        else:\n",
        "           \n",
        "           # Error handling for non-200 responses\n",
        "            print(f\"Error: La API devolvió el código {response.status_code}\")\n",
        "            print(f\"Respuesta: {response.text}\")\n",
        "            print(\"Saliendo del bucle debido a un error de la API.\")\n",
        "            break \n",
        "\n",
        "    # Error handling for connection issues\n",
        "    except requests.exceptions.RequestException as e: \n",
        "        print(f\"Error de conexión o red: {e}\")\n",
        "        print(f\"No se pudo conectar a '{BASE_URL}'. Saliendo del bucle.\")\n",
        "        break # Salimos si hay un error de conexión\n",
        "\n",
        "    # To avoid hitting rate limits, we can add a small delay between requests\n",
        "    #time.sleep(0.5)\n",
        "\n",
        "print(\"¡Datos de entrenamiento obtenidos con éxito!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk svgling\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install medspacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import medspacy\n",
        "from medspacy.ner import TargetRule\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "from loguru import logger\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "#------------------------------------------[NLP PIPELINE SETUP]------------------------------------------------\n",
        "\n",
        "# Clear logs from MedSpaCy and spaCy to reduce clutter\n",
        "logger.disable(\"PyRuSH\")\n",
        "logger.disable(\"medspacy\")\n",
        "\n",
        "# Initialize base spacy model \n",
        "base_nlp = spacy.load(\"en_core_web_sm\")  # o \"es_core_news_sm\" si es español\n",
        "\n",
        "# Merge of medspacy with base spacy model\n",
        "nlp = medspacy.load(enable=[\"target_matcher\", \"context\"], nlp=base_nlp)\n",
        "\n",
        "print(\"Pipeline después de cargar MedSpaCy:\", nlp.pipe_names)\n",
        "\n",
        "# ------------------------------------------[TARGET RULES DEFINITION]------------------------------------------------\n",
        "target_rules = [\n",
        "\n",
        "    # To detect markers\n",
        "    TargetRule(\"HbA1c\", \"MARKER\"),\n",
        "    TargetRule(\"BMI\", \"BMI\"),\n",
        "    TargetRule(\"glucose\", \"GLUCOSE\"),\n",
        "    TargetRule(\"year\", \"AGE\"),\n",
        "    TargetRule(\"female\", \"GENDER\"),\n",
        "    TargetRule(\"male\", \"GENDER\"),\n",
        "\n",
        "    # To detect smoking status\n",
        "    TargetRule(\"smoker\", \"SMOKE\"),\n",
        "    TargetRule(\"smoke\", \"SMOKE\"),\n",
        "    TargetRule(\"smoking\", \"SMOKE\"),\n",
        "    TargetRule(\"smokin\", \"SMOKE\"),\n",
        "\n",
        "    # To detect hypertension\n",
        "    TargetRule(\"hypertension\", \"HYPERTENSION\"),\n",
        "    TargetRule(\"hypertensive\", \"HYPERTENSION\"),\n",
        "    TargetRule(\"high blood pressure\", \"HYPERTENSION\"),\n",
        "    TargetRule(\"HTN\", \"HYPERTENSION\"),\n",
        "\n",
        "    # To detect heart disease\n",
        "    TargetRule(\"heart disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"coronary artery disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"ischemic heart disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"cardiovascular disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"CVD\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"IHD\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"CAD\", \"HEART_DISEASE\"),\n",
        "]\n",
        "\n",
        "# Add the target rules to the medspacy target matcher\n",
        "nlp.get_pipe(\"medspacy_target_matcher\").add(target_rules)\n",
        "\n",
        "# ------------------------------------------[CUSTOM COMPONENT DEFINITION]------------------------------------------------\n",
        "\n",
        "# Aux function to check span overlaps\n",
        "def span_overlaps_any(span_start, span_end, ents):\n",
        "    \"\"\"Devuelve True si el span [span_start, span_end) solapa con alguna entidad en ents.\"\"\"\n",
        "    for e in ents:\n",
        "        # entidad e cubre [e.start, e.end)\n",
        "        if not (span_end <= e.start or span_start >= e.end):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Custom component to find marker values safely in both directions with overlap checks\n",
        "@Language.component(\"find_marker_value_bidirectional_safe\")\n",
        "def find_marker_value_bidirectional_safe(doc):\n",
        "    current_ents = list(doc.ents)  # copy of current entities to iterate over\n",
        "    new_ents = [] # list of new entities to add\n",
        "\n",
        "    # Track occupied token indices to avoid overlaps\n",
        "    occupied_tokens = set()\n",
        "    for e in current_ents:\n",
        "        occupied_tokens.update(range(e.start, e.end))\n",
        "\n",
        "    # Iterate over current entities to find values\n",
        "    for ent in current_ents:\n",
        "        if ent.label_ not in [\"MARKER\", \"BMI\", \"GLUCOSE\", \"AGE\"]:\n",
        "            continue\n",
        "\n",
        "        # For each entity, search forwards for a value that matches criteria\n",
        "        window_start = ent.end\n",
        "        window_end = min(ent.end + 5, len(doc))\n",
        "        for token in doc[window_start:window_end]:\n",
        "          \n",
        "            start = token.i\n",
        "            if token.i - 1 >= 0 and doc[token.i - 1].pos_ == \"ADV\":\n",
        "                start = token.i - 1\n",
        "            end = token.i + 1\n",
        "\n",
        "           \n",
        "            is_value = token.like_num or token.pos_ == \"ADJ\" or token.lower_ in {\"high\",\"low\",\"normal\",\"elevated\",\"increased\",\"decreased\"}\n",
        "            if not is_value:\n",
        "                continue\n",
        "\n",
        "            \n",
        "            if span_overlaps_any(start, end, current_ents):\n",
        "                continue\n",
        "\n",
        "            \n",
        "            if span_overlaps_any(start, end, new_ents):\n",
        "                continue\n",
        "\n",
        "            # Add new entity\n",
        "            new_ents.append(Span(doc, start, end, label=f\"{ent.label_}_VALUE\"))\n",
        "            break\n",
        "\n",
        "       # Now search backwards for a value if not found forwards \n",
        "        window_start_back = max(ent.start - 5, 0)\n",
        "        window_end_back = ent.start\n",
        "       \n",
        "       # Search backwards for a value that matches criteria for each entity\n",
        "        for token in reversed(doc[window_start_back:window_end_back]):\n",
        "            start = token.i\n",
        "            \n",
        "        \n",
        "            if token.i - 1 >= 0 and doc[token.i - 1].pos_ == \"ADV\":\n",
        "                start = token.i - 1\n",
        "            end = token.i + 1\n",
        "\n",
        "            is_value = token.like_num or token.pos_ == \"ADJ\" or token.lower_ in {\"high\",\"low\",\"normal\",\"elevated\",\"increased\",\"decreased\"}\n",
        "            if not is_value:\n",
        "                continue\n",
        "\n",
        "            \n",
        "            if span_overlaps_any(start, end, current_ents):\n",
        "                continue\n",
        "            if span_overlaps_any(start, end, new_ents):\n",
        "                continue\n",
        "\n",
        "            new_ents.append(Span(doc, start, end, label=f\"{ent.label_}_VALUE\"))\n",
        "            break\n",
        "\n",
        "    # Additional logic to find GLUCOSE_UNITS after GLUCOSE_VALUE\n",
        "    extra_ents = []\n",
        "    for ent in new_ents:\n",
        "        if ent.label_ == \"GLUCOSE_VALUE\":\n",
        "            try:\n",
        "                float(ent.text)\n",
        "                \n",
        "\n",
        "                possible_units = {\"mg/dl\", \"mg/dL\", \"mg / dL\", \"mmol/L\", \"mmol/l\", \"g/L\", \"mg%\", \"mg dl\", \"mg per dL\", \"mg\"}\n",
        "                \n",
        "               \n",
        "                if ent.end < len(doc):\n",
        "                    next_token = doc[ent.end]\n",
        "                \n",
        "                    \n",
        "                    combined = next_token.text\n",
        "                   \n",
        "                    if ent.end + 2 < len(doc):\n",
        "                        combined2 = next_token.text + doc[ent.end + 1].text + doc[ent.end + 2].text\n",
        "                        combined2 = combined2.replace(\" \", \"\")\n",
        "                    else:\n",
        "                        combined2 = \"\"\n",
        "                \n",
        "                   # Normalize and check\n",
        "                    combined = combined.lower().replace(\" \", \"\")\n",
        "                    if combined in possible_units or combined2.lower() in possible_units:\n",
        "                       \n",
        "                        new_label = \"GLUCOSE_UNITS\"\n",
        "                        extra_ents.append(Span(doc, next_token.i, min(len(doc), next_token.i + 3), label=new_label))\n",
        "\n",
        "            except ValueError:\n",
        "                pass\n",
        "    \n",
        "    # Combine all entities and filter spans to avoid overlaps\n",
        "    all_ents = current_ents + new_ents + extra_ents\n",
        "    try:\n",
        "        doc.ents = filter_spans(all_ents)\n",
        "\n",
        "    # Detailed diagnostic in case of error for debugging: print problematic spans\n",
        "    except Exception as e:\n",
        "       \n",
        "        print(\"ERROR al asignar doc.ents:\", e)\n",
        "        print(\"Entidades actuales:\")\n",
        "        for e0 in current_ents:\n",
        "            print(f\"  - {e0.text} [{e0.start},{e0.end}) {e0.label_}\")\n",
        "        print(\"Entidades nuevas propuestas:\")\n",
        "        for e1 in new_ents:\n",
        "            print(f\"  - {e1.text} [{e1.start},{e1.end}) {e1.label_}\")\n",
        "        \n",
        "        raise\n",
        "\n",
        "    return doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from medspacy.context import ConTextRule\n",
        "import re\n",
        "\n",
        "#-------------------------------------------[PIPELINE CUSTOMIZATION]------------------------------------------------\n",
        "if \"medspacy_context\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"medspacy_context\", after=\"medspacy_target_matcher\")\n",
        "else:\n",
        "    nlp.remove_pipe(\"medspacy_context\")\n",
        "    nlp.add_pipe(\"medspacy_context\", after=\"medspacy_target_matcher\")\n",
        "\n",
        "#--------------------------------------------[CONTEXT RULES DEFINITION]------------------------------------------------\n",
        "context = nlp.get_pipe(\"medspacy_context\")\n",
        "context.add([\n",
        "    # Generic useful negations\n",
        "    ConTextRule(\"no\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"without\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"free of\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"denies\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"denies any\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"never\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"no history of\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=10),\n",
        "    ConTextRule(\"negative for\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "\n",
        "    # “non …” (typical variations)\n",
        "    ConTextRule(\"non\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=5),\n",
        "    ConTextRule(\"non-\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=5),\n",
        "    ConTextRule(\"non smoking\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non-smoking\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non smoker\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non-smoker\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non smokin\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "\n",
        "    # Historical smoking status indicators\n",
        "    ConTextRule(\"past\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"former\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"formerly\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"ex-smoker\", \"HISTORICAL\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"history of\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"hx of\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"h/o\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"PMH of\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "])\n",
        "\n",
        "#--------------------------------------------------[SMOKING STATUS CLASSIFIER (regrex expressions)]--------------------------------------------------\n",
        "NEG_SMOKE_RE = re.compile(\n",
        "    r\"\\b(non[-\\s]smok\\w|never\\s+smok\\w*|denies\\s+smok\\w*|no\\s+(history\\s+of\\s+)?smok\\w*|not\\s+a\\s+smoker)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "HIST_SMOKE_RE = re.compile(\n",
        "    r\"\\b(former(ly)?\\s+smok\\w*|past\\s+smok\\w*|ex[-\\s]smok\\w|history\\s+of\\s+smok\\w*)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "POS_SMOKE_RE = re.compile(\n",
        "    r\"\\b(current(ly)?\\s+a?\\s*smok\\w*|smokes\\b|smoking\\b|is\\s+a\\s+smoker|smoker\\b)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# Custom component to classify smoking status based on regex patterns\n",
        "@Language.component(\"smoking_flag_classifier\")\n",
        "def smoking_flag_classifier(doc):\n",
        "    text = doc.text.lower()\n",
        "    doc._.smoking_vote = None  # -1=neg, 0=hist, 1=pos, None=indeterminado\n",
        "\n",
        "    if NEG_SMOKE_RE.search(text):\n",
        "        doc._.smoking_vote = -1\n",
        "        return doc\n",
        "    if HIST_SMOKE_RE.search(text):\n",
        "        doc._.smoking_vote = 1   \n",
        "        return doc\n",
        "    if POS_SMOKE_RE.search(text):\n",
        "        if not NEG_SMOKE_RE.search(text):\n",
        "            doc._.smoking_vote = 1\n",
        "            return doc\n",
        "    return doc\n",
        "\n",
        "# Register the custom Doc extension for smoking vote\n",
        "if not spacy.tokens.Doc.has_extension(\"smoking_vote\"):\n",
        "    spacy.tokens.Doc.set_extension(\"smoking_vote\", default=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add custom components to the pipeline\n",
        "nlp.add_pipe(\"find_marker_value_bidirectional_safe\", after=\"medspacy_target_matcher\")\n",
        "nlp.add_pipe(\"smoking_flag_classifier\", after=\"medspacy_context\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#----------------------------------------------------------[AUXILIARY FUNCTIONS FOR FLAGS]----------------------------------------------------------\n",
        "# Smoke (current): require NOT negated, NOT family, NOT historical, NOT hypothetical\n",
        "def is_asserted_current(ent):\n",
        "    return not getattr(ent._, \"is_negated\", False) \\\n",
        "        and not getattr(ent._, \"is_family\", False) \\\n",
        "        and not getattr(ent._, \"is_historical\", False) \\\n",
        "        and not getattr(ent._, \"is_hypothetical\", False)\n",
        "\n",
        "# Illnesses (hypertension, heart disease): count historical as present if NOT negated, NOT family, NOT hypothetical\n",
        "def is_present_condition(ent):\n",
        "    return not getattr(ent._, \"is_negated\", False) \\\n",
        "        and not getattr(ent._, \"is_family\", False) \\\n",
        "        and not getattr(ent._, \"is_hypothetical\", False)\n",
        "\n",
        "# ---------------------------------------------------------[CALCULATION OF FLAGS (combines vote and ConText entities)]---------------------------------------------------------\n",
        "def compute_smoker_flag(doc):\n",
        "    # Prioridad al voto del regex si existe\n",
        "    if doc._.smoking_vote == -1:\n",
        "        return 0\n",
        "    if doc._.smoking_vote == 0:\n",
        "        return 0\n",
        "    if doc._.smoking_vote == 1:\n",
        "        return 1\n",
        "    # if there was no vote, use entities + ConText (only current)\n",
        "    return 1 if any(ent.label_ == \"SMOKE\" and is_asserted_current(ent) for ent in doc.ents) else 0\n",
        "\n",
        "def compute_hypertension_flag(doc):\n",
        "    # Account historical as present if NOT negated/family/hypothetical\n",
        "    return 1 if any(ent.label_ == \"HYPERTENSION\" and is_present_condition(ent) for ent in doc.ents) else 0\n",
        "\n",
        "def compute_heart_disease_flag(doc):\n",
        "    # Account historical as present if NOT negated/family/hypothetical\n",
        "    return 1 if any(ent.label_ == \"HEART_DISEASE\" and is_present_condition(ent) for ent in doc.ents) else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Pipeline actualizado:\", nlp.pipe_names)\n",
        "\n",
        "#------------------------------------------[TESTING ON PATIENTS]------------------------------------------------\n",
        "for i in range(len(lista)):\n",
        "    if lista[i][0]==4941:\n",
        "        text = lista[i][2]\n",
        "        doc = nlp(text)\n",
        "        print(f\"Textos encontrados en el paciente con ID {lista[i][0]}:\")\n",
        "        for ent in doc.ents:\n",
        "            print(f\"Texto: '{ent.text}', Etiqueta: '{ent.label_}'\\n\")\n",
        "    \n",
        "        smoker_flag = compute_smoker_flag(doc)\n",
        "        hypertension_flag = compute_hypertension_flag(doc)\n",
        "        heart_disease_flag = compute_heart_disease_flag(doc)\n",
        "    \n",
        "        print(f\"--> Fumador: {smoker_flag}\")\n",
        "        print(f\"--> Hypertension: {hypertension_flag}\")\n",
        "        print(f\"--> Heart disease: {heart_disease_flag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from word2number import w2n\n",
        "\n",
        "#------------------------------------------[DATA NORMALIZATION AND EXPORT]------------------------------------------------\n",
        "\n",
        "data_rows = []\n",
        "\n",
        "for i in range(len(lista)):\n",
        "    patient_id = lista[i][0]\n",
        "    has_diabetes = lista[i][1]\n",
        "    text = lista[i][2]\n",
        "    doc = nlp(text)\n",
        "\n",
        "    smoker_flag = compute_smoker_flag(doc)\n",
        "    hypertension_flag = compute_hypertension_flag(doc)\n",
        "    heart_disease_flag = compute_heart_disease_flag(doc)\n",
        "    \n",
        "    # Create a dictionary with the ID and then fill it with entities\n",
        "    row = {\"patient_id\": patient_id, \"has_diabetes\": has_diabetes, \"smoker\": smoker_flag, \"hypertension\": hypertension_flag, \"heart_disease\": heart_disease_flag}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        label = ent.label_\n",
        "        value = ent.text\n",
        "\n",
        "        # if the label is not already stored, we add it (so it is not overwritten if it already has value)\n",
        "        if label not in row and label in [\"GENDER\", \"BMI_VALUE\", \"MARKER_VALUE\", \"GLUCOSE_VALUE\", \"GLUCOSE_UNITS\"]:\n",
        "            row[label] = value\n",
        "        if label not in row and label in [\"AGE_VALUE\"]:\n",
        "            if not isinstance(value, (int, float)):\n",
        "                row[label] = w2n.word_to_num(value)\n",
        "            else:\n",
        "                row[label] = value\n",
        "    data_rows.append(row)\n",
        "\n",
        "# Convert to DataFrame (columns will be created automatically)\n",
        "df = pd.DataFrame(data_rows)\n",
        "\n",
        "# -------------------------------------[ NORMALIZATION MAPS AND FUNCTIONS ]------------------------------------------------\n",
        "bmi_map = {\n",
        "    \"low\": 16,\n",
        "    \"decreased\": 16\n",
        "    \"normal\": 22.5,\n",
        "    \"high\": 30,\n",
        "    \"increased\": 30,\n",
        "    \"elevated\": 30\n",
        "}\n",
        "\n",
        "marker_map = {\n",
        "    \"low\": 4,\n",
        "    \"decreased\": 4,\n",
        "    \"normal\": 5,\n",
        "    \"high\": 7,\n",
        "    \"increased\": 7,\n",
        "    \"elevated\": 7\n",
        "}\n",
        "\n",
        "glucose_map = {\n",
        "    \"low\": 100,\n",
        "    \"low\": 100,\n",
        "    \"normal\": 150,\n",
        "    \"high\": 250,\n",
        "    \"increased\": 250,\n",
        "    \"elevated\": 250\n",
        "}\n",
        "\n",
        "# Aux function to convert values based on mapping or numeric conversion\n",
        "def convert_value(val, mapping):\n",
        "    \"\"\"\n",
        "    Convierte texto según el mapeo. \n",
        "    Si ya es numérico o convertible, devuelve el número.\n",
        "    \"\"\"\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "    val_str = str(val).strip().lower()\n",
        "    # Si es texto conocido → asignar número\n",
        "    if val_str in mapping:\n",
        "        return mapping[val_str]\n",
        "    # Si es número → devolver como float\n",
        "    try:\n",
        "        return float(val)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "# Apply conversions\n",
        "df[\"BMI_VALUE\"] = df[\"BMI_VALUE\"].apply(lambda x: convert_value(x, bmi_map))\n",
        "df[\"MARKER_VALUE\"] = df[\"MARKER_VALUE\"].apply(lambda x: convert_value(x, marker_map))\n",
        "df[\"GLUCOSE_VALUE\"] = df[\"GLUCOSE_VALUE\"].apply(lambda x: convert_value(x, glucose_map))\n",
        "\n",
        "# Export\n",
        "df.to_csv(\"entidades_por_paciente.csv\", index=False, sep=\";\")\n",
        "print(\" CSV generado: entidades_por_paciente.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Entrenamiento del Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset to train the model\n",
        "df = pd.read_csv(\"/kaggle/input/train-2/dataset_2.csv\", sep=\";\")\n",
        "\n",
        "# Eliminate columns that do not provide information\n",
        "df = df.drop(columns=[\"patient_id\", \"GLUCOSE_UNITS\"])\n",
        "\n",
        "# Codex the categorical variable GENDER\n",
        "if \"GENDER\" in df.columns:\n",
        "    df[\"GENDER\"] = LabelEncoder().fit_transform(df[\"GENDER\"].astype(str))\n",
        "\n",
        "# Conversion of all possible columns to numeric format (non-numeric values are transformed to NaN)\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "#  Fill missing values with the mean of each column numeric\n",
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "\n",
        "#  Separate independent variables (X) from the target variable (y)\n",
        "X_train = df.drop(columns=[\"has_diabetes\"])\n",
        "y_train = df[\"has_diabetes\"]\n",
        "\n",
        "#  Divide the dataset into training and testing sets (80% train, 20% test)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0, random_state=42)\n",
        "\n",
        "#  Train the Random Forest model with defined parameters\n",
        "clf = RandomForestClassifier(n_estimators=431, min_samples_leaf=1, min_samples_split=8, max_depth=8, random_state=0, max_features=None)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#  Test the model and print accuracy \n",
        "#accuracy = clf.score(X_test, y_test)\n",
        "#print(f\" Precisión en test = {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Show detailed classification report\n",
        "#y_pred = clf.predict(X_test)\n",
        "#print(\"\\nReporte de clasificación:\")\n",
        "#print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Carga de datos del conjunto de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time \n",
        "\n",
        "BASE_URL = \"https://api.hackupm2025.workers.dev\"\n",
        "train_list_endpoint = \"/api/v1/patients/test\"\n",
        "i = 1\n",
        "lista=[]\n",
        "while True:\n",
        "    try:\n",
        "        params_consulta = {\n",
        "            'page': i,\n",
        "            'limit': 20,\n",
        "        }\n",
        "\n",
        "        url_completa = BASE_URL + train_list_endpoint\n",
        "\n",
        "        response = requests.get(url_completa, params=params_consulta, timeout=10)\n",
        "\n",
        "      \n",
        "        if response.status_code == 200:\n",
        "            datos = response.json()\n",
        "           \n",
        "            for fila in datos['data']:\n",
        "                lista.append((fila['patient_id'], fila['medical_note']))\n",
        "            \n",
        "            if not datos[\"pagination\"][\"hasNextPage\"]:\n",
        "                print(\"No hay más páginas. Saliendo del bucle.\")\n",
        "                break \n",
        "            \n",
        "           \n",
        "            i += 1\n",
        "\n",
        "        else:\n",
        "           \n",
        "            print(f\"Error: La API devolvió el código {response.status_code}\")\n",
        "            print(f\"Respuesta: {response.text}\")\n",
        "            print(\"Saliendo del bucle debido a un error de la API.\")\n",
        "            break \n",
        "\n",
        "    \n",
        "    except requests.exceptions.RequestException as e: \n",
        "        print(f\"Error de conexión o red: {e}\")\n",
        "        print(f\"No se pudo conectar a '{BASE_URL}'. Saliendo del bucle.\")\n",
        "        break \n",
        "\n",
        "   \n",
        "    #time.sleep(0.5)\n",
        "\n",
        "print(\"¡Datos de prueba obtenidos con éxito!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Aplicación del NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from word2number import w2n\n",
        "\n",
        "data_rows = []\n",
        "\n",
        "for i in range(len(lista)):\n",
        "    patient_id = lista[i][0]\n",
        "    text = lista[i][1]\n",
        "    doc = nlp(text)\n",
        "\n",
        "    smoker_flag = compute_smoker_flag(doc)\n",
        "    hypertension_flag = compute_hypertension_flag(doc)\n",
        "    heart_disease_flag = compute_heart_disease_flag(doc)\n",
        "    \n",
        "   \n",
        "    row = {\"patient_id\": patient_id, \"smoker\": smoker_flag, \"hypertension\": hypertension_flag, \"heart_disease\": heart_disease_flag}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        label = ent.label_\n",
        "        value = ent.text\n",
        "\n",
        "        \n",
        "        if label not in row and label in [\"GENDER\", \"BMI_VALUE\", \"MARKER_VALUE\", \"GLUCOSE_VALUE\", \"GLUCOSE_UNITS\"]:\n",
        "            row[label] = value\n",
        "        if label not in row and label in [\"AGE_VALUE\"]:\n",
        "            if not isinstance(value, (int, float)):\n",
        "                row[label] = w2n.word_to_num(value)\n",
        "            else:\n",
        "                row[label] = value\n",
        "    data_rows.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame(data_rows)\n",
        "\n",
        "\n",
        "bmi_map = {\n",
        "    \"low\": 16,\n",
        "    \"decreased\": 16,\n",
        "    \"normal\": 22.5,\n",
        "    \"high\": 30,\n",
        "    \"increased\": 30,\n",
        "    \"elevated\": 30\n",
        "}\n",
        "\n",
        "marker_map = {\n",
        "    \"low\": 4,\n",
        "    \"decreased\": 4,\n",
        "    \"normal\": 5,\n",
        "    \"high\": 7,\n",
        "    \"increased\": 7,\n",
        "    \"elevated\": 7\n",
        "}\n",
        "\n",
        "glucose_map = {\n",
        "    \"low\": 100,\n",
        "    \"low\": 100,\n",
        "    \"normal\": 150,\n",
        "    \"high\": 250,\n",
        "    \"increased\": 250,\n",
        "    \"elevated\": 250\n",
        "}\n",
        "\n",
        "def convert_value(val, mapping):\n",
        "    \"\"\"\n",
        "    Convert text according to mapping. \n",
        "    If it is already numeric or convertible, return the number.\n",
        "    \"\"\"\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "    val_str = str(val).strip().lower()\n",
        "   \n",
        "    if val_str in mapping:\n",
        "        return mapping[val_str]\n",
        "    \n",
        "    try:\n",
        "        return float(val)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "df[\"BMI_VALUE\"] = df[\"BMI_VALUE\"].apply(lambda x: convert_value(x, bmi_map))\n",
        "df[\"MARKER_VALUE\"] = df[\"MARKER_VALUE\"].apply(lambda x: convert_value(x, marker_map))\n",
        "df[\"GLUCOSE_VALUE\"] = df[\"GLUCOSE_VALUE\"].apply(lambda x: convert_value(x, glucose_map))\n",
        "\n",
        "\n",
        "df.to_csv(\"test.csv\", index=False, sep=\";\")\n",
        "print(\" CSV generado: test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inferencia del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Read the CSV file using ';' as separator and load it into a DataFrame\n",
        "df = pd.read_csv(\"/kaggle/input/test-1/test.csv\", sep=\";\")\n",
        "\n",
        "# Extract the 'patient_id' column and convert it to a list for later use\n",
        "patient_ids = df[\"patient_id\"].tolist()\n",
        "\n",
        "# If the column 'GENDER' exists, encode it into numeric values using LabelEncoder\n",
        "if \"GENDER\" in df.columns:\n",
        "    df[\"GENDER\"] = LabelEncoder().fit_transform(df[\"GENDER\"].astype(str))\n",
        "\n",
        "# Convert all columns to numeric format\n",
        "# Non-numeric values will be converted to NaN\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Fill missing values (NaN) with the mean of each numeric column\n",
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "\n",
        "# Prepare the test set by dropping columns that should not be used for prediction\n",
        "X_test = df.drop(columns=[\"patient_id\", \"GLUCOSE_UNITS\"])\n",
        "\n",
        "# Predict using the pre-trained classifier (clf)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Create a DataFrame with the prediction results:\n",
        "# Format patient IDs with leading zeros and include the prediction\n",
        "pred_df = pd.DataFrame({\n",
        "    \"patient_id\": [f\"patient_{str(p).zfill(5)}\" for p in patient_ids],\n",
        "    \"has_diabetes\": y_pred\n",
        "})\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
