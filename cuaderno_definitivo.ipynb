{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time \n",
        "\n",
        "BASE_URL = \"https://api.hackupm2025.workers.dev\"\n",
        "train_list_endpoint = \"/api/v1/patients/train\"\n",
        "i = 1\n",
        "lista=[]\n",
        "while True:\n",
        "    try:\n",
        "        params_consulta = {\n",
        "            'page': i,\n",
        "            'limit': 20,\n",
        "        }\n",
        "\n",
        "        url_completa = BASE_URL + train_list_endpoint\n",
        "\n",
        "        response = requests.get(url_completa, params=params_consulta, timeout=10)\n",
        "\n",
        "        # 5. Comprueba si la petici√≥n fue exitosa (c√≥digo 200)\n",
        "        if response.status_code == 200:\n",
        "            datos = response.json()\n",
        "            # print(f\"URL final solicitada: {response.url}\")\n",
        "            for fila in datos['data']:\n",
        "                lista.append((fila['patient_id'], fila['has_diabetes'], fila['medical_note']))\n",
        "            # (Opcional) Muestra cu√°ntos datos reales vinieron\n",
        "            # Asumiendo que los datos est√°n en una clave 'data')\n",
        "            \n",
        "            # ---\n",
        "            # 1. CORRECCI√ìN DE SINTAXIS Y L√ìGICA:\n",
        "            #    Mueve el 'break' DENTRO del if de √©xito.\n",
        "            #    Corrige la sintaxis de acceso al diccionario.\n",
        "            #    Compara con el booleano 'False', no con el string \"false\".\n",
        "            # ---\n",
        "            if not datos[\"pagination\"][\"hasNextPage\"]:\n",
        "                print(\"No hay m√°s p√°ginas. Saliendo del bucle.\")\n",
        "                break # ¬°√âxito! Salimos del bucle.\n",
        "            \n",
        "            # Si llegamos aqu√≠, es que hay m√°s p√°ginas. Incrementamos.\n",
        "            i += 1\n",
        "\n",
        "        else:\n",
        "            # 2. CORRECCI√ìN DE ERROR:\n",
        "            #    Si la API da un error (ej. 404, 500), debemos parar el bucle.\n",
        "            print(f\"Error: La API devolvi√≥ el c√≥digo {response.status_code}\")\n",
        "            print(f\"Respuesta: {response.text}\")\n",
        "            print(\"Saliendo del bucle debido a un error de la API.\")\n",
        "            break # Salimos del bucle si la API falla\n",
        "\n",
        "    # 3. CORRECCI√ìN DE EXCEPCI√ìN:\n",
        "    #    La sintaxis 'Exception or ...' es incorrecta.\n",
        "    #    Es mejor capturar la excepci√≥n base de 'requests'.\n",
        "    except requests.exceptions.RequestException as e: \n",
        "        print(f\"Error de conexi√≥n o red: {e}\")\n",
        "        print(f\"No se pudo conectar a '{BASE_URL}'. Saliendo del bucle.\")\n",
        "        break # Salimos si hay un error de conexi√≥n\n",
        "\n",
        "    # A√±ade una peque√±a pausa para no saturar la API\n",
        "    #time.sleep(0.5)\n",
        "\n",
        "print(\"¬°Datos de entrenamiento obtenidos con √©xito!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk svgling\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install medspacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import medspacy\n",
        "from medspacy.ner import TargetRule\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "from loguru import logger\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "logger.disable(\"PyRuSH\")\n",
        "logger.disable(\"medspacy\")\n",
        "\n",
        "# ---  Cargar modelo base de spaCy con POS tagging ---\n",
        "base_nlp = spacy.load(\"en_core_web_sm\")  # o \"es_core_news_sm\" si es espa√±ol\n",
        "\n",
        "# ---  Integrar MedSpaCy SOBRE ese modelo y guardar el resultado ---\n",
        "nlp = medspacy.load(enable=[\"target_matcher\", \"context\"], nlp=base_nlp)\n",
        "\n",
        "print(\"Pipeline despu√©s de cargar MedSpaCy:\", nlp.pipe_names)\n",
        "\n",
        "# ---  A√±adir tus reglas de entidades cl√≠nicas ---\n",
        "target_rules = [\n",
        "    TargetRule(\"HbA1c\", \"MARKER\"),\n",
        "    TargetRule(\"BMI\", \"BMI\"),\n",
        "    TargetRule(\"glucose\", \"GLUCOSE\"),\n",
        "    TargetRule(\"year\", \"AGE\"),\n",
        "    TargetRule(\"female\", \"GENDER\"),\n",
        "    TargetRule(\"male\", \"GENDER\"),\n",
        "\n",
        "    # Para determinar fumadores\n",
        "    TargetRule(\"smoker\", \"SMOKE\"),\n",
        "    TargetRule(\"smoke\", \"SMOKE\"),\n",
        "    TargetRule(\"smoking\", \"SMOKE\"),\n",
        "    TargetRule(\"smokin\", \"SMOKE\"),\n",
        "\n",
        "    # Hipertensi√≥n\n",
        "    TargetRule(\"hypertension\", \"HYPERTENSION\"),\n",
        "    TargetRule(\"hypertensive\", \"HYPERTENSION\"),\n",
        "    TargetRule(\"high blood pressure\", \"HYPERTENSION\"),\n",
        "    TargetRule(\"HTN\", \"HYPERTENSION\"),\n",
        "\n",
        "    TargetRule(\"heart disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"coronary artery disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"ischemic heart disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"cardiovascular disease\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"CVD\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"IHD\", \"HEART_DISEASE\"),\n",
        "    TargetRule(\"CAD\", \"HEART_DISEASE\"),\n",
        "]\n",
        "\n",
        "nlp.get_pipe(\"medspacy_target_matcher\").add(target_rules)\n",
        "\n",
        "def span_overlaps_any(span_start, span_end, ents):\n",
        "    \"\"\"Devuelve True si el span [span_start, span_end) solapa con alguna entidad en ents.\"\"\"\n",
        "    for e in ents:\n",
        "        # entidad e cubre [e.start, e.end)\n",
        "        if not (span_end <= e.start or span_start >= e.end):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# ---  Tu componente que busca valores adjetivales ---\n",
        "@Language.component(\"find_marker_value_bidirectional_safe\")\n",
        "def find_marker_value_bidirectional_safe(doc):\n",
        "    current_ents = list(doc.ents)  # entidades originales\n",
        "    new_ents = []\n",
        "\n",
        "    # √≠ndices ocupados por entidades originales (r√°pido para checks)\n",
        "    occupied_tokens = set()\n",
        "    for e in current_ents:\n",
        "        occupied_tokens.update(range(e.start, e.end))\n",
        "\n",
        "    for ent in current_ents:\n",
        "        if ent.label_ not in [\"MARKER\", \"BMI\", \"GLUCOSE\", \"AGE\"]:\n",
        "            continue\n",
        "\n",
        "        # -------- B√öSQUEDA HACIA ADELANTE --------\n",
        "        window_start = ent.end\n",
        "        window_end = min(ent.end + 5, len(doc))\n",
        "        for token in doc[window_start:window_end]:\n",
        "            # candidate span: [token.i, token.i+1) o incluir adv antes si procede\n",
        "            start = token.i\n",
        "            if token.i - 1 >= 0 and doc[token.i - 1].pos_ == \"ADV\":\n",
        "                start = token.i - 1\n",
        "            end = token.i + 1\n",
        "\n",
        "            # comprobar condiciones pos/num\n",
        "            is_value = token.like_num or token.pos_ == \"ADJ\" or token.lower_ in {\"high\",\"low\",\"normal\",\"elevated\",\"increased\",\"decreased\"}\n",
        "            if not is_value:\n",
        "                continue\n",
        "\n",
        "            # NO crear si solapa con entidades existentes\n",
        "            if span_overlaps_any(start, end, current_ents):\n",
        "                # si solapa, saltamos (no intentamos recortar autom√°ticamente)\n",
        "                continue\n",
        "\n",
        "            # NO solapar con nuevas entidades que ya hemos a√±adido\n",
        "            if span_overlaps_any(start, end, new_ents):\n",
        "                continue\n",
        "\n",
        "            new_ents.append(Span(doc, start, end, label=f\"{ent.label_}_VALUE\"))\n",
        "            break\n",
        "\n",
        "        # -------- B√öSQUEDA HACIA ATR√ÅS --------\n",
        "        window_start_back = max(ent.start - 5, 0)\n",
        "        window_end_back = ent.start\n",
        "        # iteramos en orden inverso para pillar el adjetivo m√°s cercano\n",
        "        for token in reversed(doc[window_start_back:window_end_back]):\n",
        "            start = token.i\n",
        "            # incluir adv antes si hay (ej. \"very high\")\n",
        "            if token.i - 1 >= 0 and doc[token.i - 1].pos_ == \"ADV\":\n",
        "                start = token.i - 1\n",
        "            end = token.i + 1\n",
        "\n",
        "            is_value = token.like_num or token.pos_ == \"ADJ\" or token.lower_ in {\"high\",\"low\",\"normal\",\"elevated\",\"increased\",\"decreased\"}\n",
        "            if not is_value:\n",
        "                continue\n",
        "\n",
        "            # evitar solapamientos con entidades originales/nuevas\n",
        "            if span_overlaps_any(start, end, current_ents):\n",
        "                continue\n",
        "            if span_overlaps_any(start, end, new_ents):\n",
        "                continue\n",
        "\n",
        "            new_ents.append(Span(doc, start, end, label=f\"{ent.label_}_VALUE\"))\n",
        "            break\n",
        "\n",
        "    # Busca las unidades de la concentraci√≥n de glucosa\n",
        "    extra_ents = []\n",
        "    for ent in new_ents:\n",
        "        if ent.label_ == \"GLUCOSE_VALUE\":\n",
        "            try:\n",
        "                float(ent.text)\n",
        "                #print(f\"'{ent.text}' es un n√∫mero float.\")\n",
        "\n",
        "                possible_units = {\"mg/dl\", \"mg/dL\", \"mg / dL\", \"mmol/L\", \"mmol/l\", \"g/L\", \"mg%\", \"mg dl\", \"mg per dL\", \"mg\"}\n",
        "                \n",
        "                # Despu√©s de detectar que 'ent.text' es un n√∫mero:\n",
        "                if ent.end < len(doc):\n",
        "                    next_token = doc[ent.end]\n",
        "                \n",
        "                    # üîç Intentamos varias formas:\n",
        "                    combined = next_token.text\n",
        "                    # incluye tambi√©n dos tokens seguidos (\"mg\" + \"/\" + \"dL\")\n",
        "                    if ent.end + 2 < len(doc):\n",
        "                        combined2 = next_token.text + doc[ent.end + 1].text + doc[ent.end + 2].text\n",
        "                        combined2 = combined2.replace(\" \", \"\")\n",
        "                    else:\n",
        "                        combined2 = \"\"\n",
        "                \n",
        "                    # Normalizar a min√∫sculas y sin espacios\n",
        "                    combined = combined.lower().replace(\" \", \"\")\n",
        "                    if combined in possible_units or combined2.lower() in possible_units:\n",
        "                        #print(f\" ‚Üí Se detecta unidad '{combined}'\")\n",
        "                        new_label = \"GLUCOSE_UNITS\"\n",
        "                        extra_ents.append(Span(doc, next_token.i, min(len(doc), next_token.i + 3), label=new_label))\n",
        "\n",
        "            except ValueError:\n",
        "                # No es float\n",
        "                pass\n",
        "    \n",
        "    # Combina y filtra solapamientos (filter_spans tambi√©n ayuda si hay igualdad/duplas)\n",
        "    all_ents = current_ents + new_ents + extra_ents\n",
        "    try:\n",
        "        doc.ents = filter_spans(all_ents)\n",
        "    except Exception as e:\n",
        "        # diagn√≥stico detallado para depuraci√≥n: imprime spans problem√°ticos\n",
        "        print(\"ERROR al asignar doc.ents:\", e)\n",
        "        print(\"Entidades actuales:\")\n",
        "        for e0 in current_ents:\n",
        "            print(f\"  - {e0.text} [{e0.start},{e0.end}) {e0.label_}\")\n",
        "        print(\"Entidades nuevas propuestas:\")\n",
        "        for e1 in new_ents:\n",
        "            print(f\"  - {e1.text} [{e1.start},{e1.end}) {e1.label_}\")\n",
        "        # re-raise para que no se silencie\n",
        "        raise\n",
        "\n",
        "    return doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from medspacy.context import ConTextRule\n",
        "import re\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Asegurar ConText justo despu√©s del target matcher\n",
        "# ---------------------------------------------------\n",
        "if \"medspacy_context\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"medspacy_context\", after=\"medspacy_target_matcher\")\n",
        "else:\n",
        "    nlp.remove_pipe(\"medspacy_context\")\n",
        "    nlp.add_pipe(\"medspacy_context\", after=\"medspacy_target_matcher\")\n",
        "\n",
        "# ------------------------------------\n",
        "# Reglas ConText extra para la estructura de las medical notes\n",
        "# ------------------------------------\n",
        "context = nlp.get_pipe(\"medspacy_context\")\n",
        "context.add([\n",
        "    # Negaciones gen√©ricas √∫tiles\n",
        "    ConTextRule(\"no\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"without\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"free of\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"denies\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"denies any\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"never\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"no history of\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=10),\n",
        "    ConTextRule(\"negative for\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=6),\n",
        "\n",
        "    # ‚Äúnon ‚Ä¶‚Äù (variaciones t√≠picas)\n",
        "    ConTextRule(\"non\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=5),\n",
        "    ConTextRule(\"non-\", \"NEGATED_EXISTENCE\", direction=\"FORWARD\", max_scope=5),\n",
        "    ConTextRule(\"non smoking\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non-smoking\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non smoker\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non-smoker\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"non smokin\", \"NEGATED_EXISTENCE\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "\n",
        "    # Hist√≥rico (no actual) ‚Äî cuenta como presente para ENFERMEDADES pero no para SMOKE\n",
        "    ConTextRule(\"past\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"former\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"formerly\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=6),\n",
        "    ConTextRule(\"ex-smoker\", \"HISTORICAL\", direction=\"BIDIRECTIONAL\", max_scope=3),\n",
        "    ConTextRule(\"history of\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"hx of\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"h/o\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "    ConTextRule(\"PMH of\", \"HISTORICAL\", direction=\"FORWARD\", max_scope=8),\n",
        "])\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# votaci√≥n por regex del SMOKE en funci√≥n de lo detectado\n",
        "# ----------------------------------------------------\n",
        "NEG_SMOKE_RE = re.compile(\n",
        "    r\"\\b(non[-\\s]smok\\w|never\\s+smok\\w*|denies\\s+smok\\w*|no\\s+(history\\s+of\\s+)?smok\\w*|not\\s+a\\s+smoker)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "HIST_SMOKE_RE = re.compile(\n",
        "    r\"\\b(former(ly)?\\s+smok\\w*|past\\s+smok\\w*|ex[-\\s]smok\\w|history\\s+of\\s+smok\\w*)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "POS_SMOKE_RE = re.compile(\n",
        "    r\"\\b(current(ly)?\\s+a?\\s*smok\\w*|smokes\\b|smoking\\b|is\\s+a\\s+smoker|smoker\\b)\\b\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "@Language.component(\"smoking_flag_classifier\")\n",
        "def smoking_flag_classifier(doc):\n",
        "    text = doc.text.lower()\n",
        "    doc._.smoking_vote = None  # -1=neg, 0=hist, 1=pos, None=indeterminado\n",
        "\n",
        "    if NEG_SMOKE_RE.search(text):\n",
        "        doc._.smoking_vote = -1\n",
        "        return doc\n",
        "    if HIST_SMOKE_RE.search(text):\n",
        "        doc._.smoking_vote = 1   \n",
        "        return doc\n",
        "    if POS_SMOKE_RE.search(text):\n",
        "        if not NEG_SMOKE_RE.search(text):\n",
        "            doc._.smoking_vote = 1\n",
        "            return doc\n",
        "    return doc\n",
        "\n",
        "# Registrar extensi√≥n de Doc para el voto\n",
        "if not spacy.tokens.Doc.has_extension(\"smoking_vote\"):\n",
        "    spacy.tokens.Doc.set_extension(\"smoking_vote\", default=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp.add_pipe(\"find_marker_value_bidirectional_safe\", after=\"medspacy_target_matcher\")\n",
        "nlp.add_pipe(\"smoking_flag_classifier\", after=\"medspacy_context\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------\n",
        "# 8) Funciones auxiliares para los flags\n",
        "# --------------------------------------------\n",
        "# Para SMOKE (actual): requerimos NO negado, NO familiar, NO hist√≥rico, NO hipot√©tico\n",
        "def is_asserted_current(ent):\n",
        "    return not getattr(ent._, \"is_negated\", False) \\\n",
        "        and not getattr(ent._, \"is_family\", False) \\\n",
        "        and not getattr(ent._, \"is_historical\", False) \\\n",
        "        and not getattr(ent._, \"is_hypothetical\", False)\n",
        "\n",
        "# Para ENFERMEDADES: contar como presente si NO est√° negado, NO familiar, NO hipot√©tico\n",
        "def is_present_condition(ent):\n",
        "    return not getattr(ent._, \"is_negated\", False) \\\n",
        "        and not getattr(ent._, \"is_family\", False) \\\n",
        "        and not getattr(ent._, \"is_hypothetical\", False)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 9) C√°lculo de flags (combina voto y entidades ConText)\n",
        "# ----------------------------------------------------------\n",
        "def compute_smoker_flag(doc):\n",
        "    # Prioridad al voto del regex si existe\n",
        "    if doc._.smoking_vote == -1:\n",
        "        return 0\n",
        "    if doc._.smoking_vote == 0:\n",
        "        return 0\n",
        "    if doc._.smoking_vote == 1:\n",
        "        return 1\n",
        "    # Si no hubo voto, usa entidades + ConText (solo actuales)\n",
        "    return 1 if any(ent.label_ == \"SMOKE\" and is_asserted_current(ent) for ent in doc.ents) else 0\n",
        "\n",
        "def compute_hypertension_flag(doc):\n",
        "    # Cuenta hist√≥rico como presente mientras no est√© negado/familiar/hipot√©tico\n",
        "    return 1 if any(ent.label_ == \"HYPERTENSION\" and is_present_condition(ent) for ent in doc.ents) else 0\n",
        "\n",
        "def compute_heart_disease_flag(doc):\n",
        "    # Cuenta hist√≥rico como presente mientras no est√© negado/familiar/hipot√©tico\n",
        "    return 1 if any(ent.label_ == \"HEART_DISEASE\" and is_present_condition(ent) for ent in doc.ents) else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Pipeline actualizado:\", nlp.pipe_names)\n",
        "\n",
        "for i in range(len(lista)):\n",
        "    if lista[i][0]==4941:\n",
        "        text = lista[i][2]\n",
        "        doc = nlp(text)\n",
        "        print(f\"Textos encontrados en el paciente con ID {lista[i][0]}:\")\n",
        "        for ent in doc.ents:\n",
        "            print(f\"Texto: '{ent.text}', Etiqueta: '{ent.label_}'\\n\")\n",
        "    \n",
        "        smoker_flag = compute_smoker_flag(doc)\n",
        "        hypertension_flag = compute_hypertension_flag(doc)\n",
        "        heart_disease_flag = compute_heart_disease_flag(doc)\n",
        "    \n",
        "        print(f\"--> Fumador: {smoker_flag}\")\n",
        "        print(f\"--> Hypertension: {hypertension_flag}\")\n",
        "        print(f\"--> Heart disease: {heart_disease_flag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from word2number import w2n\n",
        "\n",
        "data_rows = []\n",
        "\n",
        "for i in range(len(lista)):\n",
        "    patient_id = lista[i][0]\n",
        "    has_diabetes = lista[i][1]\n",
        "    text = lista[i][2]\n",
        "    doc = nlp(text)\n",
        "\n",
        "    smoker_flag = compute_smoker_flag(doc)\n",
        "    hypertension_flag = compute_hypertension_flag(doc)\n",
        "    heart_disease_flag = compute_heart_disease_flag(doc)\n",
        "    \n",
        "    # Creamos un diccionario con el ID y luego rellenamos con las entidades\n",
        "    row = {\"patient_id\": patient_id, \"has_diabetes\": has_diabetes, \"smoker\": smoker_flag, \"hypertension\": hypertension_flag, \"heart_disease\": heart_disease_flag}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        label = ent.label_\n",
        "        value = ent.text\n",
        "\n",
        "        # Si esa etiqueta a√∫n no est√° guardada, la a√±adimos\n",
        "        # (as√≠ no se sobrescribe si ya tiene valor)\n",
        "        if label not in row and label in [\"GENDER\", \"BMI_VALUE\", \"MARKER_VALUE\", \"GLUCOSE_VALUE\", \"GLUCOSE_UNITS\"]:\n",
        "            row[label] = value\n",
        "        if label not in row and label in [\"AGE_VALUE\"]:\n",
        "            if not isinstance(value, (int, float)):\n",
        "                row[label] = w2n.word_to_num(value)\n",
        "            else:\n",
        "                row[label] = value\n",
        "    data_rows.append(row)\n",
        "\n",
        "# Convertimos a DataFrame (las columnas se crear√°n autom√°ticamente)\n",
        "df = pd.DataFrame(data_rows)\n",
        "\n",
        "# --- Mapeos definidos ---\n",
        "bmi_map = {\n",
        "    \"low\": 16,\n",
        "    \"decreased\": 16\n",
        "    \"normal\": 22.5,\n",
        "    \"high\": 30,\n",
        "    \"increased\": 30,\n",
        "    \"elevated\": 30\n",
        "}\n",
        "\n",
        "marker_map = {\n",
        "    \"low\": 4,\n",
        "    \"decreased\": 4,\n",
        "    \"normal\": 5,\n",
        "    \"high\": 7,\n",
        "    \"increased\": 7,\n",
        "    \"elevated\": 7\n",
        "}\n",
        "\n",
        "glucose_map = {\n",
        "    \"low\": 100,\n",
        "    \"low\": 100,\n",
        "    \"normal\": 150,\n",
        "    \"high\": 250,\n",
        "    \"increased\": 250,\n",
        "    \"elevated\": 250\n",
        "}\n",
        "\n",
        "def convert_value(val, mapping):\n",
        "    \"\"\"\n",
        "    Convierte texto seg√∫n el mapeo. \n",
        "    Si ya es num√©rico o convertible, devuelve el n√∫mero.\n",
        "    \"\"\"\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "    val_str = str(val).strip().lower()\n",
        "    # Si es texto conocido ‚Üí asignar n√∫mero\n",
        "    if val_str in mapping:\n",
        "        return mapping[val_str]\n",
        "    # Si es n√∫mero ‚Üí devolver como float\n",
        "    try:\n",
        "        return float(val)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "# --- Aplicar conversiones ---\n",
        "df[\"BMI_VALUE\"] = df[\"BMI_VALUE\"].apply(lambda x: convert_value(x, bmi_map))\n",
        "df[\"MARKER_VALUE\"] = df[\"MARKER_VALUE\"].apply(lambda x: convert_value(x, marker_map))\n",
        "df[\"GLUCOSE_VALUE\"] = df[\"GLUCOSE_VALUE\"].apply(lambda x: convert_value(x, glucose_map))\n",
        "\n",
        "# Exportamos\n",
        "df.to_csv(\"entidades_por_paciente.csv\", index=False, sep=\";\")\n",
        "print(\" CSV generado: entidades_por_paciente.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Entrenamiento del Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "#  # Cargar el archivo CSV\n",
        "df = pd.read_csv(\"/kaggle/input/train-2/dataset_2.csv\", sep=\";\")\n",
        "\n",
        "#  # Eliminar columnas que no aportan informaci√≥n\n",
        "df = df.drop(columns=[\"patient_id\", \"GLUCOSE_UNITS\"])\n",
        "\n",
        "#  # Codificar la variable categ√≥rica GENDER\n",
        "if \"GENDER\" in df.columns:\n",
        "    df[\"GENDER\"] = LabelEncoder().fit_transform(df[\"GENDER\"].astype(str))\n",
        "\n",
        "#  # Convertir todas las columnas posibles a formato num√©rico (valores no num√©ricos se transforman en NaN)\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "#  # Rellenar los valores faltantes con la media de cada columna num√©rica\n",
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "\n",
        "#  # Separar las variables independientes (X) de la variable objetivo (y)\n",
        "X_train = df.drop(columns=[\"has_diabetes\"])\n",
        "y_train = df[\"has_diabetes\"]\n",
        "\n",
        "#  # Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0, random_state=42)\n",
        "\n",
        "#  # Entrenar el modelo Random Forest con los par√°metros definidos\n",
        "clf = RandomForestClassifier(n_estimators=431, min_samples_leaf=1, min_samples_split=8, max_depth=8, random_state=0, max_features=None)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#  # Evaluar el rendimiento del modelo con el conjunto de prueba\n",
        "#accuracy = clf.score(X_test, y_test)\n",
        "#print(f\" Precisi√≥n en test = {accuracy * 100:.2f}%\")\n",
        "\n",
        "# # Mostrar un informe completo de clasificaci√≥n si se desea\n",
        "#y_pred = clf.predict(X_test)\n",
        "#print(\"\\nReporte de clasificaci√≥n:\")\n",
        "#print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Carga de datos del conjunto de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time # Buena pr√°ctica a√±adir una pausa\n",
        "\n",
        "BASE_URL = \"https://api.hackupm2025.workers.dev\"\n",
        "train_list_endpoint = \"/api/v1/patients/test\"\n",
        "i = 1\n",
        "lista=[]\n",
        "while True:\n",
        "    try:\n",
        "        params_consulta = {\n",
        "            'page': i,\n",
        "            'limit': 20,\n",
        "        }\n",
        "\n",
        "        url_completa = BASE_URL + train_list_endpoint\n",
        "\n",
        "        response = requests.get(url_completa, params=params_consulta, timeout=10)\n",
        "\n",
        "        # 5. Comprueba si la petici√≥n fue exitosa (c√≥digo 200)\n",
        "        if response.status_code == 200:\n",
        "            datos = response.json()\n",
        "            # print(f\"URL final solicitada: {response.url}\")\n",
        "            for fila in datos['data']:\n",
        "                lista.append((fila['patient_id'], fila['medical_note']))\n",
        "            # (Opcional) Muestra cu√°ntos datos reales vinieron\n",
        "            # Asumiendo que los datos est√°n en una clave 'data')\n",
        "            \n",
        "            # ---\n",
        "            # 1. CORRECCI√ìN DE SINTAXIS Y L√ìGICA:\n",
        "            #    Mueve el 'break' DENTRO del if de √©xito.\n",
        "            #    Corrige la sintaxis de acceso al diccionario.\n",
        "            #    Compara con el booleano 'False', no con el string \"false\".\n",
        "            # ---\n",
        "            if not datos[\"pagination\"][\"hasNextPage\"]:\n",
        "                print(\"No hay m√°s p√°ginas. Saliendo del bucle.\")\n",
        "                break # ¬°√âxito! Salimos del bucle.\n",
        "            \n",
        "            # Si llegamos aqu√≠, es que hay m√°s p√°ginas. Incrementamos.\n",
        "            i += 1\n",
        "\n",
        "        else:\n",
        "            # 2. CORRECCI√ìN DE ERROR:\n",
        "            #    Si la API da un error (ej. 404, 500), debemos parar el bucle.\n",
        "            print(f\"Error: La API devolvi√≥ el c√≥digo {response.status_code}\")\n",
        "            print(f\"Respuesta: {response.text}\")\n",
        "            print(\"Saliendo del bucle debido a un error de la API.\")\n",
        "            break # Salimos del bucle si la API falla\n",
        "\n",
        "    # 3. CORRECCI√ìN DE EXCEPCI√ìN:\n",
        "    #    La sintaxis 'Exception or ...' es incorrecta.\n",
        "    #    Es mejor capturar la excepci√≥n base de 'requests'.\n",
        "    except requests.exceptions.RequestException as e: \n",
        "        print(f\"Error de conexi√≥n o red: {e}\")\n",
        "        print(f\"No se pudo conectar a '{BASE_URL}'. Saliendo del bucle.\")\n",
        "        break # Salimos si hay un error de conexi√≥n\n",
        "\n",
        "    # A√±ade una peque√±a pausa para no saturar la API\n",
        "    #time.sleep(0.5)\n",
        "\n",
        "print(\"¬°Datos de prueba obtenidos con √©xito!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Aplicaci√≥n del NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from word2number import w2n\n",
        "\n",
        "data_rows = []\n",
        "\n",
        "for i in range(len(lista)):\n",
        "    patient_id = lista[i][0]\n",
        "    text = lista[i][1]\n",
        "    doc = nlp(text)\n",
        "\n",
        "    smoker_flag = compute_smoker_flag(doc)\n",
        "    hypertension_flag = compute_hypertension_flag(doc)\n",
        "    heart_disease_flag = compute_heart_disease_flag(doc)\n",
        "    \n",
        "    # Creamos un diccionario con el ID y luego rellenamos con las entidades\n",
        "    row = {\"patient_id\": patient_id, \"smoker\": smoker_flag, \"hypertension\": hypertension_flag, \"heart_disease\": heart_disease_flag}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        label = ent.label_\n",
        "        value = ent.text\n",
        "\n",
        "        # Si esa etiqueta a√∫n no est√° guardada, la a√±adimos\n",
        "        # (as√≠ no se sobrescribe si ya tiene valor)\n",
        "        if label not in row and label in [\"GENDER\", \"BMI_VALUE\", \"MARKER_VALUE\", \"GLUCOSE_VALUE\", \"GLUCOSE_UNITS\"]:\n",
        "            row[label] = value\n",
        "        if label not in row and label in [\"AGE_VALUE\"]:\n",
        "            if not isinstance(value, (int, float)):\n",
        "                row[label] = w2n.word_to_num(value)\n",
        "            else:\n",
        "                row[label] = value\n",
        "    data_rows.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertimos a DataFrame (las columnas se crear√°n autom√°ticamente)\n",
        "df = pd.DataFrame(data_rows)\n",
        "\n",
        "# --- Mapeos definidos ---\n",
        "bmi_map = {\n",
        "    \"low\": 16,\n",
        "    \"decreased\": 16,\n",
        "    \"normal\": 22.5,\n",
        "    \"high\": 30,\n",
        "    \"increased\": 30,\n",
        "    \"elevated\": 30\n",
        "}\n",
        "\n",
        "marker_map = {\n",
        "    \"low\": 4,\n",
        "    \"decreased\": 4,\n",
        "    \"normal\": 5,\n",
        "    \"high\": 7,\n",
        "    \"increased\": 7,\n",
        "    \"elevated\": 7\n",
        "}\n",
        "\n",
        "glucose_map = {\n",
        "    \"low\": 100,\n",
        "    \"low\": 100,\n",
        "    \"normal\": 150,\n",
        "    \"high\": 250,\n",
        "    \"increased\": 250,\n",
        "    \"elevated\": 250\n",
        "}\n",
        "\n",
        "def convert_value(val, mapping):\n",
        "    \"\"\"\n",
        "    Convierte texto seg√∫n el mapeo. \n",
        "    Si ya es num√©rico o convertible, devuelve el n√∫mero.\n",
        "    \"\"\"\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "    val_str = str(val).strip().lower()\n",
        "    # Si es texto conocido ‚Üí asignar n√∫mero\n",
        "    if val_str in mapping:\n",
        "        return mapping[val_str]\n",
        "    # Si es n√∫mero ‚Üí devolver como float\n",
        "    try:\n",
        "        return float(val)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "# --- Aplicar conversiones ---\n",
        "df[\"BMI_VALUE\"] = df[\"BMI_VALUE\"].apply(lambda x: convert_value(x, bmi_map))\n",
        "df[\"MARKER_VALUE\"] = df[\"MARKER_VALUE\"].apply(lambda x: convert_value(x, marker_map))\n",
        "df[\"GLUCOSE_VALUE\"] = df[\"GLUCOSE_VALUE\"].apply(lambda x: convert_value(x, glucose_map))\n",
        "\n",
        "# Exportamos\n",
        "df.to_csv(\"test.csv\", index=False, sep=\";\")\n",
        "print(\" CSV generado: test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inferencia del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/kaggle/input/test-1/test.csv\", sep=\";\")\n",
        "\n",
        "patient_ids = df[\"patient_id\"].tolist()\n",
        "\n",
        "if \"GENDER\" in df.columns:\n",
        "    df[\"GENDER\"] = LabelEncoder().fit_transform(df[\"GENDER\"].astype(str))\n",
        "\n",
        "#  # Convertir todas las columnas posibles a formato num√©rico (valores no num√©ricos se transforman en NaN)\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "#  # Rellenar los valores faltantes con la media de cada columna num√©rica\n",
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "\n",
        "X_test = df.drop(columns=[\"patient_id\", \"GLUCOSE_UNITS\"])\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "pred_df = pd.DataFrame({\n",
        "    \"patient_id\": [f\"patient_{str(p).zfill(5)}\" for p in patient_ids],\n",
        "    \"has_diabetes\": y_pred\n",
        "})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
